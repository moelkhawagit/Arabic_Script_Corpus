{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LangIdentification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2WJocD-6Dst"
      },
      "source": [
        "# Arabic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "zMcN23Ijwmx_",
        "outputId": "df5e409d-61ce-4bda-f9f7-21982c6785c2"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/arabic.csv\")\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lang</th>\n",
              "      <th>text</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ar</td>\n",
              "      <td>['اللُّغَة العَرَبِيّة هي أكثر اللغات السامية ...</td>\n",
              "      <td>اللغة العربية</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ar</td>\n",
              "      <td>['إثيوبيا ( رسمياً، جمهورية إثيوبيا الفيدرالية...</td>\n",
              "      <td>إثيوبيا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ar</td>\n",
              "      <td>['اللغات الساميّة هي أحد فروع اللغات الأفروآسي...</td>\n",
              "      <td>لغات سامية</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ar</td>\n",
              "      <td>['السنغال (بالفرنسية: le Sénégal)\\u200f، رسميً...</td>\n",
              "      <td>السنغال</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ar</td>\n",
              "      <td>['تشاد (بالفرنسية: Tchad)\\u200f رسمياً جمهورية...</td>\n",
              "      <td>تشاد</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  lang                                               text          title\n",
              "0   ar  ['اللُّغَة العَرَبِيّة هي أكثر اللغات السامية ...  اللغة العربية\n",
              "1   ar  ['إثيوبيا ( رسمياً، جمهورية إثيوبيا الفيدرالية...        إثيوبيا\n",
              "2   ar  ['اللغات الساميّة هي أحد فروع اللغات الأفروآسي...     لغات سامية\n",
              "3   ar  ['السنغال (بالفرنسية: le Sénégal)\\u200f، رسميً...        السنغال\n",
              "4   ar  ['تشاد (بالفرنسية: Tchad)\\u200f رسمياً جمهورية...           تشاد"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWHi086NxcE7",
        "outputId": "694443d5-3b25-4b2c-e2c0-2873e663d473"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsBXYxJY0MHS"
      },
      "source": [
        "## Split articles into sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH-Uag7uz6Jj",
        "outputId": "cda7f1ec-e313-4ca9-f93c-e5b5390918fd"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize \n",
        "\n",
        "sentences = df['text'].apply(sent_tokenize)\n",
        "print(sentences[0][1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "وبذلك فهي تحتل المركز الرابع أو الخامس من حيث اللغات الأكثر انتشاراً في العالم،[6][7][8] واللغة الرابعة من حيث عدد المستخدمين على الإنترنت.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zipsAPEX0T2T"
      },
      "source": [
        "## Preprocess sentences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juKs3uaP1hpJ"
      },
      "source": [
        "tokenized_sent = []\n",
        "\n",
        "for par in sentences:\n",
        "  for sent in par:\n",
        "    tokenized_sent.append(nltk.word_tokenize(sent))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bEtSvjWo3dG_",
        "outputId": "2b65d095-326b-419e-c7e7-b9a71dc13c7d"
      },
      "source": [
        "tokenized_sent[0][1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'اللُّغَة\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-oQjdzq0GPh",
        "outputId": "d28b8838-ae07-4e92-bffc-4b9c97da99b3"
      },
      "source": [
        "# REMOVES STOPWORDS\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('arabic'))\n",
        "for sent in tokenized_sent:\n",
        "    for w in sent:\n",
        "        if w in stop_words:\n",
        "            sent.remove(w)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQrbNlVI0z_J",
        "outputId": "67c09f99-47c8-4c51-8f80-7130f98e7cb3"
      },
      "source": [
        "# REMOVE NON ALPHA CHARS\n",
        "import string\n",
        "punct = string.punctuation\n",
        "for sent in tokenized_sent:\n",
        "    for w in sent:\n",
        "         if not(w.isalpha()) :\n",
        "            sent.remove(w)\n",
        "print(sentences[5:25])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5     [['الأهواز أو الأحواز هي عاصمة ومركز محافظة خو...\n",
            "6     [['إريتريا، أو رسمياً دولة إريتريا هي بلد في ا...\n",
            "7     [['مالي أو جمهورية مالي وهي دولة غير ساحلية في...\n",
            "8     [['السواحلية كما تُعرف باسم اللسان السواحلي هي...\n",
            "9     [['تُرْكِيَا (بالتركية: Türkiye)\\u200f الاِسم ...\n",
            "10    [['الهَوْسية (بالهوسية: هَرْشَن هَوْسَ) لغة ال...\n",
            "11    [['اللغة الألبانية (بالألبانية: Gjuha shqipe)....\n",
            "12    [['اللغة الإندونيسية هي اللغة الرسمية لإندونيس...\n",
            "13    [['اللغة الماليزية (الملايو: bahasa Malaysia؛ ...\n",
            "14    [['اللغة الأردية أو لغة لاشكاري[3] لغة هندية آ...\n",
            "15                     [['تشير كلمة كردية إلى\\xa0:\\n']]\n",
            "16    [['الوطن العربي (ويُعرف كذلك باسم الوطن العربي...\n",
            "17    [['القرن الأفريقي أو شبه الجزيرة الصومالية[1][...\n",
            "18    [['نَسَمة هي وحدة تعداد الأشخاص., تستعمل على س...\n",
            "19    [['تعدد الآلهة هو مصطلح يشير إلى الإيمان بآلهة...\n",
            "20    [['نجد هي أحد أقاليم شبه الجزيرة العربية التار...\n",
            "21    [['التوراة (بالعبرية: תּוֹרָה؛ تعني الشريعة أو...\n",
            "22    [['عدنان بن أدد هو جد العرب العدنانيين المشترك...\n",
            "23                                                 [[]]\n",
            "24    [['مضر الجد السابع عشر للنبي محمد بن عبد الله،...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksl6PrdK4Lhz",
        "outputId": "198c8aca-a3ae-4257-940d-41850a96cfe1"
      },
      "source": [
        "import re \n",
        "for sent in tokenized_sent:\n",
        "    for w in sent:\n",
        "         if re.search('[a-zA-Z]', w) or re.search(\"[._@z'+#-?&!/,]\", w):\n",
        "            sent.remove(w)\n",
        "print(tokenized_sent[5])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['وللغة', 'العربية', 'تأثير', 'مباشر', 'وغير', 'مباشر', 'كثير', 'اللغات', 'الأخرى', 'العالم', 'كالتركية', 'والفارسية', 'والأمازيغية', 'والكردية', 'والأردية', 'والماليزية', 'والإندونيسية', 'والألبانية', 'وبعض', 'اللغات', 'الإفريقية', 'الأخرى', 'مثل', 'الهاوسا', 'والسواحيلية', 'والتجرية', 'والأمهرية', 'وبعض', 'اللغات', 'الأوروبية', 'المتوسطية', 'كالإسبانية', 'والبرتغالية', 'والمالطية', 'ودخلت', 'الكثير', 'مصطلحاتها', 'اللغة', 'الإنجليزية', 'واللغات', 'مثل', 'أدميرال', 'والتعريفة', 'والكحول', 'والجبر', 'وأسماء', 'النجوم']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpK3avUK4q9v",
        "outputId": "eecfa429-8571-491d-970a-40f574155bc5"
      },
      "source": [
        "len(tokenized_sent)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215321"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Q7Ezt66m4y"
      },
      "source": [
        "import random\n",
        "random.seed(3)\n",
        "random.shuffle(tokenized_sent)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-W5JUHT7tgR"
      },
      "source": [
        "# Urdu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSWV4rss70bd"
      },
      "source": [
        "dfu = pd.read_csv(\"/content/drive/MyDrive/urdu.csv\")\n",
        "sentencesu = dfu['text'].apply(sent_tokenize)\n",
        "urtokenized_sent = []\n",
        "\n",
        "for par in sentencesu:\n",
        "  for sent in par:\n",
        "    urtokenized_sent.append(nltk.word_tokenize(sent))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0YH2foz-4pc",
        "outputId": "717ce29a-2785-463e-f7b1-4e4077f0624b"
      },
      "source": [
        "len(urtokenized_sent)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14475"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfRGmkWo8TId"
      },
      "source": [
        "# REMOVE NON ALPHA CHARS\n",
        "for sent in urtokenized_sent:\n",
        "    for w in sent:\n",
        "         if not(w.isalpha()) :\n",
        "            sent.remove(w)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zOXYvoX8hNA",
        "outputId": "ae572136-d2ac-4c05-fc3e-1fbf4f0fd0cd"
      },
      "source": [
        "for sent in urtokenized_sent:\n",
        "    for w in sent:\n",
        "         if re.search('[a-zA-Z]', w) or re.search(\"[._@z'+#-?&!/,]\", w):\n",
        "            sent.remove(w)\n",
        "print(urtokenized_sent[7])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['الاقوامی', 'زیادہ', 'تر', 'کا', 'مطلب', 'کوئی', 'زبان', 'یا', 'تنظیم', 'جس', 'میں', 'ایک', 'سے', 'زیادہ', 'ملک', 'شامل', 'لفظ', 'بین', 'الاقوامی', 'ایک', 'اصطلاح', 'کے', 'طور', 'پر', 'استعمال', 'ہوتا', 'جس', 'میں', 'ایک', 'سے', 'زیادہ', 'اقوام', 'یا', 'ممالک', 'یا', 'عام', 'طور', 'پر', 'قومی', 'حدود', 'سے', 'باہر', 'کے', 'کوئی', 'دیگر', 'ادارے', 'یا', 'تنظیمیں', 'ملوث', 'مثال', 'کے', 'طور', 'پر', 'بین', 'الاقوامی', 'قانون', 'ایک', 'سے', 'زیادہ', 'ممالک', 'بلکہ', 'زمین', 'پر', 'ہر', 'جگہ', 'لاگو', 'کیا', 'جاتا']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAq1pUkV8yRs"
      },
      "source": [
        "urlabel = [1]*14475"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9orw4Gq908E"
      },
      "source": [
        "# Pashto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lSglQiT94-S"
      },
      "source": [
        "dfp = pd.read_csv(\"/content/drive/MyDrive/pashto.csv\")\n",
        "sentencesp = dfp['text'].apply(sent_tokenize)\n",
        "pstokenized_sent = []\n",
        "\n",
        "for par in sentencesp:\n",
        "  for sent in par:\n",
        "    pstokenized_sent.append(nltk.word_tokenize(sent))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyUA-l9--FDp"
      },
      "source": [
        "# REMOVE NON ALPHA CHARS\n",
        "for sent in pstokenized_sent:\n",
        "    for w in sent:\n",
        "         if not(w.isalpha()) :\n",
        "            sent.remove(w)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-x7eis0-PQf",
        "outputId": "19ffcfdc-7e1a-44ae-e496-d699d508fec2"
      },
      "source": [
        "for sent in pstokenized_sent:\n",
        "    for w in sent:\n",
        "         if re.search('[a-zA-Z]', w) or re.search(\"[._@z'+#-?&!/,]\", w):\n",
        "            sent.remove(w)\n",
        "print(pstokenized_sent[6])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['چې', 'د', 'همدغې', 'سيمې', 'د', 'اوسيدونکو', 'لخوا', 'د', 'هغوی', 'په', 'ژبه', 'منصوبه', 'شوه', 'او', 'دغې', 'پوهنې', 'ته', 'د', 'الکېمي', 'نوم', 'ورکړ', 'شو']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et3o_eeF-XiT",
        "outputId": "1cef326d-afe3-4d7f-ab87-8ce08ffade9a"
      },
      "source": [
        "len(pstokenized_sent)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130651"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_uVB7bv-f-R"
      },
      "source": [
        "random.seed(3)\n",
        "random.shuffle(pstokenized_sent)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBrJ4HuX_IsV"
      },
      "source": [
        "# Farsi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6wCEsIa_MbQ"
      },
      "source": [
        "dff = pd.read_csv(\"/content/drive/MyDrive/farsi.csv\")\n",
        "sentencesf = dff['text'].apply(sent_tokenize)\n",
        "frtokenized_sent = []\n",
        "\n",
        "for par in sentencesf:\n",
        "  for sent in par:\n",
        "    frtokenized_sent.append(nltk.word_tokenize(sent))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N81pg2Vc_Yei"
      },
      "source": [
        "# REMOVE NON ALPHA CHARS\n",
        "for sent in frtokenized_sent:\n",
        "    for w in sent:\n",
        "         if not(w.isalpha()) :\n",
        "            sent.remove(w)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTaABFnK_chG",
        "outputId": "2659f487-3318-40d0-f05c-2f8f3fcd58c5"
      },
      "source": [
        "for sent in frtokenized_sent:\n",
        "    for w in sent:\n",
        "         if re.search('[a-zA-Z]', w) or re.search(\"[._@z'+#-?&!/,]\", w):\n",
        "            sent.remove(w)\n",
        "print(frtokenized_sent[6])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['در', 'زبان', 'شیمی', 'یک', 'ترانویسی', 'از', 'برابر', 'فرانسوی', 'است', 'و', 'نخستین', 'بار', 'در', 'سال', 'توسط', 'میرزا', 'صالح', 'شیرازی', 'در', 'یک', 'علوم', 'طبیعی', 'که', 'خود', 'وی', 'مرقوم', 'داشته', 'بود', 'برده', 'شد', 'که', 'بعدها', 'در', 'دارالفنون', 'با', 'عنوان', 'رسالهٔ', 'طبیعیات', 'تدریس', 'کیهان\\\\u200cشناسی\\\\nاخترشناسی', 'کهکشانی', 'اخترشناسی', 'اخترزیست\\\\u200cشناسی', 'زیستی', 'رفتاری', 'زیست\\\\u200cشناسی', 'سلولی', 'تکاملی', 'مقدمه', 'مقدمه', 'ایمنی\\\\u200cشناسی', 'دریایی', 'مولکولی', 'انگل\\\\u200cشناسی', 'زیست\\\\u200cشناسی', 'نظری', 'جانورشناسی\\\\n', 'محیط', 'معدنی\\\\nدانش', 'آلی\\\\nفوتوشیمی\\\\nشیمی\\\\u200cفیزیک\\\\nشیمی', 'حالت', 'سطح\\\\nشیمی', 'جوی', 'محیط', 'زیست', 'ژئومورفولوژی\\\\nژئوفیزیک', 'آب', 'فیزیکی', 'کاربردی', 'فیزیک', 'ماده', 'تجربی', 'ذرات', 'فیزیک', 'کوانتومی', 'مقدمه', 'جامدات', 'فیزیک', 'عام', 'باستان\\\\u200cشناسی\\\\nجرم\\\\u200cشناسی', 'جغرافیا\\\\nتاریخ', 'سیاسی', 'حقوق\\\\n', 'کشاورزی', 'مهندسی', 'هوافضا', 'مهندسی', 'پزشکی', 'مهندسی', 'شیمی', 'مهندسی', 'کامپیوتر', 'مهندسی', 'برق', 'مهندسی', 'محافظت', 'از', 'ژنتیک', 'مهندسی', 'صنعتی', 'مهندسی', 'مکانیک', 'مهندسی', 'معدن', 'مهندسی', 'مهندسی', 'زیستی', 'مراقبتهای', 'بهداشتی', 'داروسازی', 'مددکاری', \"'علوم\", 'ریاضی\\\\nآمار', \"'\", ',', 'کاربردی', 'هوش', 'بیوانفورماتیک', 'پزشکی', 'شناختی', '•', 'فرهنگی', '•', '•', 'تکاملی', '•', '•', 'کتابداری', '•', 'ریاضی', '•', 'فیزیک', '•', 'مهندسی', '•', 'اقتصاد', 'علم', 'و', 'علمی', '•', '•', '•', \"'\", ',', 'علم\\\\nفلسفه', 'علم\\\\nسیاست', \"'\", ',', 'نخستین', 'بشر', 'برای', 'فهمیدن', 'طبیعت', 'مواد', 'و', 'بیان', 'چگونگی', 'دگرگونی', 'ناموفق', 'بود']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSoKPkb__lZ_",
        "outputId": "b5ff7377-930f-423a-8603-3d367a7afc6b"
      },
      "source": [
        "len(frtokenized_sent)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "139655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX1E3nKP_vim"
      },
      "source": [
        "random.seed(3)\n",
        "random.shuffle(frtokenized_sent)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRNDZQ4eAHzw"
      },
      "source": [
        "# Form dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jgHE_iuAMKG"
      },
      "source": [
        "# For each 20k language sentence => 20k label ranging from 0 to 3\n",
        "arlabel = [0]*20000\n",
        "pslabel = [2]*20000\n",
        "frlabel = [3]*20000\n",
        "labels = arlabel+urlabel+pslabel+frlabel\n",
        "tokenized_sent = tokenized_sent[:20000] + urtokenized_sent + pstokenized_sent[20000] + frtokenized_sent[:20000]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk-tonwrCVp0"
      },
      "source": [
        "s = \" \"\n",
        "t = []\n",
        "for sent in tokenized_sent:\n",
        "  t.append(s.join(sent))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fd0cv1O3DKq_",
        "outputId": "9ca55a80-5dab-42db-c7da-880973726477"
      },
      "source": [
        "t[1]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'وحسب صندوق النقد الدولي سنة تصدرّت ستة دول أوروبية قائمة الدول الأكثر العالم حسب الناتج المحلي'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apPEEEhKVul1"
      },
      "source": [
        "# Tokenize and Padd "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyeSlyaZBoRR",
        "outputId": "7469d608-161e-418c-d866-b2499df3a8a8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=40000, oov_token= '<oov>')\n",
        "tokenizer.fit_on_texts(t)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "print(vocab_size)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "222670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zmi4ixNEVzH"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(t)\n",
        "padded = pad_sequences(sequences, maxlen= 1500, padding= 'post', truncating='post')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKFeTJ3WVz66"
      },
      "source": [
        "# Build Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0bLYoZpEgW3",
        "outputId": "01179655-caa6-4e90-eac6-3a521c1fe30e"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(50000, 32 , input_length=None),\n",
        "    tf.keras.layers.GRU(16, dropout= 0.1),\n",
        "    tf.keras.layers.Dense(16, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dense(8, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='Nadam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 32)          1600000   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 16)                2400      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 36        \n",
            "=================================================================\n",
            "Total params: 1,602,844\n",
            "Trainable params: 1,602,844\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX0E3u2FV6b0"
      },
      "source": [
        "# Train model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_bW3AKHFLFv"
      },
      "source": [
        "catg = tf.keras.utils.to_categorical(\n",
        "    labels, num_classes=4, dtype='float32'\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iafyki-FE0Lb",
        "outputId": "cdc5854a-3a8f-4d85-e9bb-ca10dd693647"
      },
      "source": [
        "num_epochs = 20\n",
        "history = model.fit(padded, catg, batch_size = 512,  epochs=num_epochs, validation_split=0.15 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "91/91 [==============================] - 172s 2s/step - loss: 1.3437 - accuracy: 0.4308 - val_loss: 1.5368 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/20\n",
            "91/91 [==============================] - 171s 2s/step - loss: 1.2073 - accuracy: 0.4314 - val_loss: 1.4913 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/20\n",
            "91/91 [==============================] - 174s 2s/step - loss: 1.1614 - accuracy: 0.4319 - val_loss: 1.4221 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/20\n",
            "91/91 [==============================] - 173s 2s/step - loss: 0.8275 - accuracy: 0.6144 - val_loss: 0.9018 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/20\n",
            "91/91 [==============================] - 173s 2s/step - loss: 0.4825 - accuracy: 0.8136 - val_loss: 0.5713 - val_accuracy: 0.9592\n",
            "Epoch 6/20\n",
            "91/91 [==============================] - 173s 2s/step - loss: 0.2355 - accuracy: 0.9818 - val_loss: 0.3419 - val_accuracy: 0.9682\n",
            "Epoch 7/20\n",
            "91/91 [==============================] - 174s 2s/step - loss: 0.1758 - accuracy: 0.9863 - val_loss: 0.3410 - val_accuracy: 0.9649\n",
            "Epoch 8/20\n",
            "91/91 [==============================] - 179s 2s/step - loss: 0.1568 - accuracy: 0.9880 - val_loss: 0.3169 - val_accuracy: 0.9702\n",
            "Epoch 9/20\n",
            "91/91 [==============================] - 179s 2s/step - loss: 0.1445 - accuracy: 0.9892 - val_loss: 0.3106 - val_accuracy: 0.9698\n",
            "Epoch 10/20\n",
            "91/91 [==============================] - 178s 2s/step - loss: 0.1359 - accuracy: 0.9895 - val_loss: 0.3104 - val_accuracy: 0.9631\n",
            "Epoch 11/20\n",
            " 8/91 [=>............................] - ETA: 2:38 - loss: 0.1261 - accuracy: 0.9915"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lp6ag5XUc1_"
      },
      "source": [
        "# As the training approaches the tenth epoch the training accuracy is about 99% and the validation accuracy is 97%!"
      ]
    }
  ]
}